{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scallop = pd.read_csv(\"scallop.csv\", usecols = [\"latitude\", \"longitude\",\"tot.catch\"])\n",
    "scallop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divided data by longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = scallop.copy()\n",
    "data[\"tot.catch\"] = np.log(scallop[\"tot.catch\"] + 1)\n",
    "# choose longitude more than -72 as testing data\n",
    "train = data[data[\"longitude\"]< -72]\n",
    "test = data[data[\"longitude\"]>= -72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(train[\"longitude\"], train[\"latitude\"], alpha=0.5,\n",
    "            c=train[\"tot.catch\"], cmap='viridis')\n",
    "plt.scatter(test[\"longitude\"], test[\"latitude\"], alpha=0.5,\n",
    "            c=test[\"tot.catch\"], cmap='viridis',  marker='x')\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"longitude\")\n",
    "plt.ylabel(\"latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2D_train = np.c_[train[\"longitude\"],  train[\"latitude\"]]\n",
    "Y_2D_train = np.array(train[\"tot.catch\"])\n",
    "\n",
    "X_2D_test = np.c_[test[\"longitude\"],  test[\"latitude\"]]\n",
    "Y_2D_test = np.array(test[\"tot.catch\"])\n",
    "\n",
    "rx = np.arange(min(data[\"longitude\"]), max(data[\"longitude\"]), 0.06)\n",
    "ry = np.arange(min(data[\"latitude\"]), max(data[\"latitude\"]), 0.06)\n",
    "gx, gy = np.meshgrid(rx, ry)\n",
    "\n",
    "\n",
    "X_2D = np.c_[gx.ravel(), gy.ravel()]\n",
    "len(X_2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp_2D(gx, gy, mu,sd, X_train, Y_train, X_test, Y_test):\n",
    "    z_min = min(min(mu), min(Y_train))\n",
    "    z_max = max(max(mu), max(Y_train))\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "    c = ax1.pcolormesh(gx, gy,mu.reshape(gx.shape), vmin = z_min,vmax = z_max, alpha=0.2,cmap='viridis')\n",
    "    ax1.scatter(X_train[:,0], X_train[:,1], alpha=0.2, c=Y_train, vmin = z_min,vmax = z_max,cmap='viridis')\n",
    "    ax1.scatter(X_test[:,0], X_test[:,1], marker='x', alpha=0.2, c=Y_test, cmap='viridis')\n",
    "    fig.colorbar(c, ax = ax1)\n",
    "    ax1.set_xlabel(\"longitude\")\n",
    "    ax1.set_ylabel(\"latitude\")\n",
    "    ax1.set_title(\"Posterior mean\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    c = ax2.pcolormesh(gx, gy, sd.reshape(gx.shape), alpha=0.2, cmap='viridis')\n",
    "    fig.colorbar(c, ax = ax2)\n",
    "    ax2.set_xlabel(\"longitude\")\n",
    "    ax2.set_ylabel(\"latitude\")\n",
    "    ax2.set_title(\"Posterior sd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process ---- Hyperparameters \n",
    "the Kernel Squared Exponential (SE) as equation 2.16 in the text book:\n",
    "<center>$kernel\\_SE = \\sigma_f^2 exp(-\\frac{1}{2l^2}|x_i - x_j|^2)$</center>\n",
    "\n",
    "The hyperparameters we are interested in are $\\sigma_n, l, \\sigma_f$:\n",
    "1. $\\sigma_f$: the scale of the output values (the overall variance of the process).\n",
    "2. l: the scale at which distances are measured among inputs (the distance from which on two points will be uncorrelated) \n",
    "3. $\\sigma_n$: the noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC using PyMC3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the prior of those three parameters (demonstrated by Stan tutorial) :\\\n",
    "https://mc-stan.org/docs/2_22/stan-users-guide/fit-gp-section.html#priors-gp.section\n",
    "1. l ~ invGamma(5,5)\n",
    "2. sigma_f ~ Normal(0, 1)\n",
    "3. sigma_n ~ Normal(0, 1)\n",
    "\n",
    "Sampling by Nuts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import theano\n",
    "theano.config.gcc.cxxflags = \"-Wno-c++11-narrowing\"\n",
    "\n",
    "# hyperparameter priors\n",
    "number_of_dim = 2\n",
    "niter =1000\n",
    "x = X_2D_train\n",
    "y = Y_2D_train\n",
    "with pm.Model() as scallop_model_1:\n",
    "    l = pm.InverseGamma(\"l\", 5, 5)\n",
    "    sigma_f = pm.Normal(\"sigma_f\", 0, 1)\n",
    "    \n",
    "\n",
    "# covariance function and marginal GP\n",
    "with scallop_model_1:\n",
    "    K = sigma_f** 2 * pm.gp.cov.ExpQuad(number_of_dim, ls = l)\n",
    "    gp = pm.gp.Marginal(cov_func=K)\n",
    "    \n",
    "# marginal likelihood\n",
    "with scallop_model_1:\n",
    "    sigma_n = pm.HalfNormal(\"sigma_n\",1)\n",
    "    tot_catch = gp.marginal_likelihood(\"tot_catch\", X = x, y = y, noise = sigma_n)\n",
    "    \n",
    "# model fitting\n",
    "with scallop_model_1:\n",
    "    trace_1 = pm.sample(niter, random_seed=123, progressbar=True, tune=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pm.summary(trace_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_2D\n",
    "# y = Y_2D_train\n",
    "with scallop_model_1:\n",
    "    scallop_pred_noisy = gp.conditional(\"scallop_pred_noisy\",X_new,pred_noise = True)\n",
    "    scallop_samples = pm.sample_posterior_predictive(trace_1, vars = [scallop_pred_noisy],samples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_1 = np.zeros(len(X_new))\n",
    "sd_1 = np.zeros(len(X_new))\n",
    "\n",
    "for i in range(0,len(X_new)):\n",
    "    mu_1[i] = np.mean(scallop_samples[\"scallop_pred_noisy\"][:,i])\n",
    "    sd_1[i] = np.std(scallop_samples[\"scallop_pred_noisy\"][:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_gp_2D(gx, gy, mu_1,sd_1, X_2D_train, Y_2D_train, X_2D_test, Y_2D_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = Y_2D_train\n",
    "with scallop_model_1:\n",
    "    scallop_pred_noisy_test = gp.conditional(\"scallop_pred_noisy_test\",X_2D_test,pred_noise = True)\n",
    "    scallop_samples_test = pm.sample_posterior_predictive(trace_1, vars = [scallop_pred_noisy_test],samples=50)\n",
    "\n",
    "mu_test_1 = np.zeros(len(X_2D_test))\n",
    "sd_test_1 = np.zeros(len(X_2D_test))\n",
    "\n",
    "for i in range(0,len(X_2D_test)):\n",
    "    mu_test_1[i] = np.mean(scallop_samples_test[\"scallop_pred_noisy_test\"][:,i])\n",
    "    mu_test_1[i] = np.std(scallop_samples_test[\"scallop_pred_noisy_test\"][:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\"pred\":mu_test_1,\n",
    "              \"true\":Y_2D_test,\n",
    "              \"lower\": mu_test_1 - 1.96 * sd_test_1,\n",
    "              \"upper\": mu_test_1 + 1.96 * sd_test_1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = np.sqrt(np.mean((np.exp(mu_test_1) - np.exp(Y_2D_test))**2))\n",
    "RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the prior of those three parameters :\n",
    "1. l ~ HalfCauchy(3)\n",
    "2. sigma_f ~ HalfCauchy(3)\n",
    "3. sigma_n ~ halfNormal(1)\n",
    "\n",
    "\n",
    "Sampling by Nuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "\n",
    "# hyperparameter priors\n",
    "number_of_dim = 2\n",
    "niter =1000\n",
    "x = X_2D_train\n",
    "y = Y_2D_train\n",
    "\n",
    "with pm.Model() as scallop_model_2:\n",
    "    l = pm.HalfCauchy(\"l\",3)\n",
    "    sigma_f = pm.HalfCauchy(\"sigma_f\",3)\n",
    "\n",
    "# covariance function and marginal GP\n",
    "with scallop_model_2:\n",
    "    K = sigma_f ** 2 * pm.gp.cov.ExpQuad(number_of_dim, ls = l)\n",
    "    gp = pm.gp.Marginal(cov_func=K)\n",
    "    \n",
    "# marginal likelihood\n",
    "with scallop_model_2:\n",
    "    sigma_n = pm.HalfNormal('sigma_n', 1)\n",
    "    tot_catch = gp.marginal_likelihood(\"tot_catch\", X = x, y = y, noise = sigma_n)\n",
    "    \n",
    "# model fitting\n",
    "with scallop_model_2:\n",
    "    trace_2 = pm.sample(niter,random_seed=123, progressbar=True, tune=500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pm.summary(trace_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_2D\n",
    "# y = Y_2D_train\n",
    "with scallop_model_2:\n",
    "    scallop_pred_noisy_2 = gp.conditional(\"scallop_pred_noisy_2\",X_new,pred_noise = True)\n",
    "    scallop_samples_2 = pm.sample_posterior_predictive(trace_2, vars = [scallop_pred_noisy_2],samples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_2 = np.zeros(len(X_new))\n",
    "sd_2 = np.zeros(len(X_new))\n",
    "\n",
    "for i in range(0,len(X_new)):\n",
    "    mu_2[i] = np.mean(scallop_samples_2[\"scallop_pred_noisy_2\"][:,i])\n",
    "    sd_2[i] = np.std(scallop_samples_2[\"scallop_pred_noisy_2\"][:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gp_2D(gx, gy, mu_2,sd_2, X_2D_train, Y_2D_train, X_2D_test, Y_2D_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = Y_2D_train\n",
    "with scallop_model_2:\n",
    "    scallop_pred_noisy_test_2 = gp.conditional(\"scallop_pred_noisy_test_2\",X_2D_test,pred_noise = True)\n",
    "    scallop_samples_test_2 = pm.sample_posterior_predictive(trace_2, vars = [scallop_pred_noisy_test_2],samples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_test_2 = np.zeros(len(X_2D_test))\n",
    "sd_test_2 = np.zeros(len(X_2D_test))\n",
    "\n",
    "for i in range(0,len(X_2D_test)):\n",
    "    mu_test_2[i] = np.mean(scallop_samples_test_2[\"scallop_pred_noisy_test_2\"][:,i])\n",
    "    sd_test_2[i] = np.std(scallop_samples_test_2[\"scallop_pred_noisy_test_2\"][:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\"pred\":mu_test_2,\n",
    "              \"true\":Y_2D_test,\n",
    "              \"lower\": mu_test_2 - 1.96 * sd_test_2,\n",
    "              \"upper\": mu_test_2 + 1.96 * sd_test_2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = np.sqrt(np.mean((np.exp(mu_test_2) - np.exp(Y_2D_test))**2))\n",
    "RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Gamma distribution and Cauchy Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import invgamma, halfcauchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace (0, 10, 200) \n",
    "y1 = halfcauchy.pdf(x, 3)\n",
    "plt.plot(x, y1, \"y-\", label=(r'$\\alpha=5, \\beta=1/5$')) \n",
    "\n",
    "plt.ylim([0,1])\n",
    "plt.xlim([0,10])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace (4, 7, 200) \n",
    "y1 = invgamma.pdf(x, a=5, loc=5)\n",
    "plt.plot(x, y1, \"y-\", label=(r'$\\alpha=5, \\beta=1/5$')) \n",
    "\n",
    "plt.ylim([0,5])\n",
    "plt.xlim([4,7])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scallop_model_1.name = \"scallop_model_1\"\n",
    "scallop_model_2.name = \"scallop_model_2\"\n",
    "df_comp_LOO = pm.compare({scallop_model_1: trace_1, scallop_model_2: trace_2}, ic='LOO')\n",
    "df_comp_LOO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE\n",
    "Posterior\n",
    "$mean = K(X_s,X)[K(X,X) + \\sigma_n^2 *I]^{-1}y$\\\n",
    "$cov = K(X_s,X_s) - K(X_s,X)[K(X,X) + \\sigma_n^2 *I]^{-1} K(X,X_s)$\\\n",
    "$logp(y|X, \\theta) = -\\frac{1}{2}y^T(K + \\sigma_n^2)^{-1}y - 1/2log(K + \\sigma_n^2) - \\frac{n}{2}log(2\\pi)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import cholesky, det, lstsq\n",
    "from scipy.optimize import minimize\n",
    "from numpy.linalg import inv\n",
    "\n",
    "def kernel(X1, X2, l=1.0, sigma_f=1.0):\n",
    "    sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "    return sigma_f**2 * np.exp(-0.5 / l**2 * sqdist)\n",
    "\n",
    "def posterior_predictive(kernel, X_s, X_train, Y_train,l=1.0, sigma_f=1.0, sigma_y=1e-8):\n",
    "    K = kernel(X_train, X_train, l, sigma_f) + sigma_y**2 * np.eye(len(X_train))\n",
    "    K_s_ = kernel(X_s, X_train, l, sigma_f)\n",
    "    K__s = kernel(X_train, X_s, l, sigma_f)\n",
    "    K_ss = kernel(X_s, X_s, l, sigma_f)\n",
    "    K_inv = inv(K)\n",
    "    \n",
    "    # Equation (4)\n",
    "    mu_s = K_s_.dot(K_inv).dot(Y_train)\n",
    "\n",
    "    # Equation (5)\n",
    "    cov_s = K_ss - K_s_.dot(K_inv).dot(K__s)\n",
    "\n",
    "    return mu_s, cov_s\n",
    "\n",
    "def nll_fn(X_train, Y_train):\n",
    "    \n",
    "    def neg_log_lik(theta):\n",
    "        # theta[0] = l\n",
    "        # theta[1] = sigma_f\n",
    "        # theta[2] = sigma_n\n",
    "        K = kernel(X_train, X_train, l=theta[0], sigma_f=theta[1]) + theta[2]**2 * np.eye(len(X_train))\n",
    "        return 0.5 * np.log(det(K)) + \\\n",
    "               0.5 * Y_train.T.dot(inv(K)).dot(Y_train) + \\\n",
    "               0.5 * len(X_train) * np.log(2*np.pi)\n",
    "    return neg_log_lik\n",
    "\n",
    "res = minimize(nll_fn(X_2D_train, Y_2D_train), [1, 1, 1], \n",
    "               bounds=((1e-5, None), (1e-5, None), (1e-5, None)),\n",
    "               method='L-BFGS-B')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_opt, sigma_f_opt, sigma_n_opt = res.x\n",
    "l_opt, sigma_f_opt, sigma_n_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute the posterior predictive statistics with optimized kernel parameters and plot the results\n",
    "mu_s, cov_s = posterior_predictive(kernel, X_2D, X_2D_train, Y_2D_train, l=l_opt, sigma_f=sigma_f_opt, sigma_y=sigma_n_opt)\n",
    "plot_gp_2D(gx, gy, mu_s,np.sqrt(np.diag(cov_s)), X_2D_train, Y_2D_train, X_2D_test, Y_2D_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_s_test, cov_s_test = posterior_predictive(kernel, X_2D_test, X_2D_train, Y_2D_train, l=l_opt, sigma_f=sigma_f_opt, sigma_y=sigma_n_opt)\n",
    "RMSE = np.sqrt(np.mean((np.exp(mu_s_test) - np.exp(Y_2D_test))**2))\n",
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_s_test, cov_s_test = posterior_predictive(kernel, X_2D_test, X_2D_train, Y_2D_train, l=1, sigma_f=1, sigma_y=0.2)\n",
    "RMSE = np.sqrt(np.mean((np.exp(mu_s_test) - np.exp(Y_2D_test))**2))\n",
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
